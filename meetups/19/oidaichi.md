# oidaichi

## 会社や業務で普段やっていること
* 画像認識
* 最適化
* データ分析
## (option) 教えられるかもしれないこと
* 機械学習アルゴリズム
* ディープラーニング
* 最適化
## 今日やること
* Kaggleコンペティションに取り組む。
## (option) もしかしたら相談するかもしれないこと
* 統計
## 今日の成果
* jupyter notebooksでソース＆可視化結果
* 相関行列で相関の1に近いor-1に近い説明変数同士は統計的手法に入れるときはなくしたほうが良い。でも機械学習はそこまででもない。
* 欠損値に対して、どういう補間をするかは大切。平均値なのか、中央値なのか最頻値なのか、データ生成するのか。
* 業務的に重要と思われるデータがない場合、説明変数を組み合わせて（足し算とか掛け算とか）生成すると効果的。
* 目的変数は正規分布に従っていると回帰しやすい。目的変数に一部大きな値がある場合、対数をとることで大きな値が丸められるため、正規分布に近づく。
* 上記にかかわって、目的変数を変更した場合、回帰が終わってからその逆数をとる必要がある。
* ランダムフォレストなどで変数重要度を出し、重要度の高い上位の変数を組み合わせて交互作用変数を作ると効果的。
* 説明変数と目的変数の散布図行列を作り、外れ値を目視で探す。外れ値があったら、マニュアルで行を削除する。
* スタッキングというものがある。